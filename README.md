# transforming-cooking

## Using extrinsic evaluations to determine whether transformers create more suitable language models for food recipes in comparison with recurrent neural nets. GaTech <3

All of the code can be found in 'Cooking_With_BERT.ipynb'.

There is a brief Setup section followed by the char-rnn model, word-rnn model, and finall the GPT-2 model. Sections are labeled with headers.

Generally the models start with their own set up, then the training and reloading, and finally the evaluation methods.

In order to run the GPT-2 section, the environment must be restarted and set to Tensorflow = 1.x
It is also important to note, that is repo lacks the checkpoints necessary to run our fine-tuned model. The checkpoint is too large for github.

As this project was done in google colab with heavy leveraging of google drive for data managment, please shoot me an email if you would like access to the original repo to test out functionality impossible in this github version.  aschott6@gatech.edu

